{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "MNIST 99.74% with Convoluted NN and Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "aa621f76195962da026e03055468e0d5ae6fca66",
        "id": "bNeZVPVPP_9o",
        "colab_type": "text"
      },
      "source": [
        "* MNIST with Convoluted NN Keras\n",
        "* Train set is made of Kaggle 42k + Keras 60k = 102k\n",
        "* CV on train and then test on 10k from Keras\n",
        "* Final model is trained on train+test = 112k\n",
        "* CNN with adam ( vs rmsprop) and dropout against overfitting\n",
        "\n",
        "* It's a quick intro to the capabilities of CNN and Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd2R2MKCq_Or",
        "colab_type": "text"
      },
      "source": [
        "# Importing Dataset form kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF3PUgQGQJiY",
        "colab_type": "code",
        "outputId": "552eb4de-1126-4c3e-e41e-515538a3d9aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "user = 'nateshreddy'\n",
        "key = 'da0bcdffa3cc7217c967c2dcd0383f84'\n",
        "\n",
        "if '.kaggle' not in os.listdir('/root'):\n",
        "    !mkdir ~/.kaggle\n",
        "!touch /root/.kaggle/kaggle.json\n",
        "!chmod 666 /root/.kaggle/kaggle.json\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    f.write('{\"username\":\"%s\",\"key\":\"%s\"}' % (user, key))\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!kaggle competitions download -c digit-recognizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!unzip \\*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv               \n",
            "\n",
            "Archive:  test.csv.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: test.csv                \n",
            "\n",
            "2 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo1qwictsRMI",
        "colab_type": "text"
      },
      "source": [
        "# Importing the necessary Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "wQaadTbmP_9s",
        "colab_type": "code",
        "outputId": "2651aba9-8093-4c59-a2e1-d1cf419ec1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# IMPORT modules\n",
        "# TURN ON the GPU !!!\n",
        "# If importing dataset from outside - like the Keras dataset - Internet must be \"connected\"\n",
        "\n",
        "import os\n",
        "from operator import itemgetter    \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "get_ipython().magic(u'matplotlib inline')\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import np_utils, to_categorical\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "print(os.getcwd())\n",
        "print(\"Modules imported \\n\")\n",
        "print(\"Files in current directory:\")\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../content\"]).decode(\"utf8\")) #check the files available in the directory"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Modules imported \n",
            "\n",
            "Files in current directory:\n",
            "sample_data\n",
            "sample_submission.csv\n",
            "test.csv\n",
            "test.csv.zip\n",
            "test.ipynb\n",
            "train.csv\n",
            "train.csv.zip\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngd9aL2HQH1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDsAD7d9QIfs",
        "colab_type": "text"
      },
      "source": [
        "# Reading the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "dd21d55df8216b41b6f62a6df369246a5c860149",
        "id": "YIxD5JIKP_9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ini = pd.read_csv('../content/train.csv')\n",
        "test_ini = pd.read_csv('../content/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "62137573d06bff5fd352ef7b2f79b88a527ed97d",
        "id": "1wDzrh1EP_93",
        "colab_type": "code",
        "outputId": "63964a42-13de-4d82-8acc-5c0cb747fe2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train = train_ini.copy()\n",
        "test_imagesKaggle = test_ini.copy()\n",
        "train_labelsKaggle = train_ini['label']\n",
        "\n",
        "print(\"train with Labels  \", train.shape)\n",
        "print(\"train_labelsKaggle \", train_labelsKaggle.shape)\n",
        "print(\"_\"*50)\n",
        "train.drop(['label'],axis=1, inplace=True) #dropping label\n",
        "train_imagesKaggle = train\n",
        "print(\"train_imagesKaggle without Labels \", train_imagesKaggle.shape)\n",
        "print(\"_\"*50)\n",
        "print(\"test_imagesKaggle  \", test_imagesKaggle.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train with Labels   (42000, 785)\n",
            "train_labelsKaggle  (42000,)\n",
            "__________________________________________________\n",
            "train_imagesKaggle without Labels  (42000, 784)\n",
            "__________________________________________________\n",
            "test_imagesKaggle   (28000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpzn349XxOLS",
        "colab_type": "text"
      },
      "source": [
        "# Reshaping Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "08c17320b4df5efce432e399afdd0b58a9272493",
        "id": "ApMqwjt8P_99",
        "colab_type": "code",
        "outputId": "6fc0667f-d2df-4d5b-a95b-26c2122c0eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "# RESHAPE to 28 X 28 (Height, Width) which Kaggle has flattened in their file\n",
        "\n",
        "train4Display = np.array(train_imagesKaggle).reshape(42000,28,28)\n",
        "test4Display = np.array(test_imagesKaggle).reshape(28000,28,28)\n",
        "\n",
        "z = 4056\n",
        "\n",
        "print(\"train image\")\n",
        "print(train_labelsKaggle[z])\n",
        "digit = train4Display[z]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()\n",
        "\n",
        "print(\"test image\")\n",
        "digit = test4Display[z]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train image\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIOyAIQg7YAjCDhjif6x/e1yq2hVtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "test image\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYNiJhGDYiYT4H70so70iTGWEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKjnO5aotYpF",
        "colab_type": "text"
      },
      "source": [
        "# NORMALIZE / SCALE and Prep for CNN in terms of number dimensions expected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c6f6371bd2c5400a5cb4a735ef8a1e61b2093d66",
        "id": "Dvi5LcM7P_-C",
        "colab_type": "code",
        "outputId": "99e39cad-970b-47a9-8813-8c2262a23345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_imagesKaggle = train4Display.reshape(42000,28,28,1)\n",
        "test_imagesKaggle = test4Display.reshape(28000,28,28,1)\n",
        "\n",
        "train_imagesKaggle = train_imagesKaggle.astype('float32') / 255\n",
        "test_imagesKaggle = test_imagesKaggle.astype('float32') / 255\n",
        "print(\"train_imagesKaggle \",train_imagesKaggle.shape)\n",
        "print(\"test_imagesKaggle \", test_imagesKaggle.shape)\n",
        "print(\"_\"*50)\n",
        "\n",
        "# ONE HOT ENCODER for the labels\n",
        "train_labelsKaggle = to_categorical(train_labelsKaggle)\n",
        "print(\"train_labelsKaggle \",train_labelsKaggle.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_imagesKaggle  (42000, 28, 28, 1)\n",
            "test_imagesKaggle  (28000, 28, 28, 1)\n",
            "__________________________________________________\n",
            "train_labelsKaggle  (42000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "efd02eff42dd1d81d3408b3e10d8aa9769c97197",
        "id": "vAIrXw65P_-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Data from Keras MNIST\n",
        "\n",
        "(train_imagesRaw, train_labelsRaw), (test_imagesRaw, test_labelsRaw) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ed31acc871c37d7386fe4aa7e791d25ae3217e5d",
        "id": "qVJtNvPtP_-O",
        "colab_type": "code",
        "outputId": "3e9072f6-698c-46fa-e9b0-642fafe8b544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Normalize / Scale and One Hot encoder for the Keras dataset & Reshape for CNN\n",
        "\n",
        "train_imagesKeras = train_imagesRaw.copy()\n",
        "train_labelsKeras = train_labelsRaw.copy()\n",
        "test_imagesKeras = test_imagesRaw.copy()\n",
        "test_labelsKeras = test_labelsRaw.copy()\n",
        "\n",
        "train_imagesKeras = train_imagesKeras.reshape(60000,28,28,1)\n",
        "test_imagesKeras = test_imagesKeras.reshape(10000,28,28,1)\n",
        "\n",
        "print(\"train_imagesKeras \",train_imagesKeras.shape)\n",
        "print(\"train_labelsKeras \",train_labelsKeras.shape)\n",
        "print(\"test_imagesKeras \", test_imagesKeras.shape)\n",
        "print(\"test_labelsKeras \", test_labelsKeras.shape)\n",
        "\n",
        "# NORMALIZE 0-255 to 0-1\n",
        "train_imagesKeras = train_imagesKeras.astype('float32') / 255\n",
        "test_imagesKeras = test_imagesKeras.astype('float32') / 255\n",
        "print(\"_\"*50)\n",
        "\n",
        "# ONE HOT ENCODER for the labels\n",
        "train_labelsKeras = to_categorical(train_labelsKeras)\n",
        "test_labelsKeras = to_categorical(test_labelsKeras)\n",
        "print(\"train_labelsKeras \",train_labelsKeras.shape)\n",
        "print(\"test_labelsKeras \", test_labelsKeras.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_imagesKeras  (60000, 28, 28, 1)\n",
            "train_labelsKeras  (60000,)\n",
            "test_imagesKeras  (10000, 28, 28, 1)\n",
            "test_labelsKeras  (10000,)\n",
            "__________________________________________________\n",
            "train_labelsKeras  (60000, 10)\n",
            "test_labelsKeras  (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whNXsY9jxa_l",
        "colab_type": "text"
      },
      "source": [
        "# Concatenating Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "76394ab509a426d75385ebf6e2f66c8c4c116edf",
        "id": "_DMp2C0JP_-T",
        "colab_type": "code",
        "outputId": "5558743e-7800-4fa4-f268-d19d48b7de92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# CONCATENATE the training sets of Kaggle and Keras into final TRAIN and leave the test for CV\n",
        "\n",
        "train_images = np.concatenate((train_imagesKeras,train_imagesKaggle), axis=0)\n",
        "print(\"new Concatenated train_images \", train_images.shape)\n",
        "print(\"_\"*50)\n",
        "\n",
        "train_labels = np.concatenate((train_labelsKeras,train_labelsKaggle), axis=0)\n",
        "print(\"new Concatenated train_labels \", train_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new Concatenated train_images  (102000, 28, 28, 1)\n",
            "__________________________________________________\n",
            "new Concatenated train_labels  (102000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuJdifCRxkee",
        "colab_type": "text"
      },
      "source": [
        "# Making the Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "15ce294fa6aee518d0def20afa1b489cda189e3c",
        "id": "uMGnAPPKP_-Y",
        "colab_type": "code",
        "outputId": "6904ff26-e5f1-4325-d196-abf32d4cc730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "# Initial model\n",
        "import keras\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(8, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "#model.add(layers.Dropout(0.1))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(12, (3, 3), activation='relu'))\n",
        "#model.add(layers.Dropout(0.2))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "model.add(layers.Dense(48, activation='relu'))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "filepath=\"weights_best.hdf5\" # checkpoint\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 12)        876       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 12)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 3, 3, 32)          3488      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 48)                1584      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                1568      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 7,926\n",
            "Trainable params: 7,926\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5szLtwYRSYfx",
        "colab_type": "text"
      },
      "source": [
        "## -Total parameters = 7,926"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exyNTQU9P_-f",
        "colab_type": "text"
      },
      "source": [
        "# Fitting and Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0361e316830d909e82a8e56fef09bca13f51bcd7",
        "id": "lP5TkexGP_-k",
        "colab_type": "code",
        "outputId": "7b9b2c68-f1a7-40ee-fdc2-bfa7a8d9fbe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initial fIT & Evaluate initial model\n",
        "\n",
        "num_epochs = 150\n",
        "BatchSize = 512\n",
        "\n",
        "final_model=model.fit(train_images, train_labels, epochs=num_epochs, batch_size=BatchSize, validation_data=(test_imagesKeras, test_labelsKeras),callbacks=[checkpoint])\n",
        "\"\"\"#test_loss, test_acc = model.evaluate(test_imagesKeras, test_labelsKeras)\n",
        "print(\"_\"*80)\n",
        "print(\"Accuracy on test \", test_acc)\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 102000 samples, validate on 10000 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "102000/102000 [==============================] - 16s 152us/step - loss: 1.2806 - acc: 0.5593 - val_loss: 0.4362 - val_acc: 0.8671\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.86710, saving model to weights_best.hdf5\n",
            "Epoch 2/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.3966 - acc: 0.8737 - val_loss: 0.2233 - val_acc: 0.9293\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.86710 to 0.92930, saving model to weights_best.hdf5\n",
            "Epoch 3/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.2636 - acc: 0.9165 - val_loss: 0.1558 - val_acc: 0.9526\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.92930 to 0.95260, saving model to weights_best.hdf5\n",
            "Epoch 4/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.2094 - acc: 0.9338 - val_loss: 0.1260 - val_acc: 0.9612\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.95260 to 0.96120, saving model to weights_best.hdf5\n",
            "Epoch 5/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.1779 - acc: 0.9443 - val_loss: 0.1170 - val_acc: 0.9625\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.96120 to 0.96250, saving model to weights_best.hdf5\n",
            "Epoch 6/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.1570 - acc: 0.9507 - val_loss: 0.0925 - val_acc: 0.9706\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.96250 to 0.97060, saving model to weights_best.hdf5\n",
            "Epoch 7/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.1411 - acc: 0.9559 - val_loss: 0.0818 - val_acc: 0.9741\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.97060 to 0.97410, saving model to weights_best.hdf5\n",
            "Epoch 8/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.1305 - acc: 0.9597 - val_loss: 0.0728 - val_acc: 0.9772\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.97410 to 0.97720, saving model to weights_best.hdf5\n",
            "Epoch 9/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.1193 - acc: 0.9631 - val_loss: 0.0689 - val_acc: 0.9789\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.97720 to 0.97890, saving model to weights_best.hdf5\n",
            "Epoch 10/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.1120 - acc: 0.9651 - val_loss: 0.0663 - val_acc: 0.9783\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.97890\n",
            "Epoch 11/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.1041 - acc: 0.9676 - val_loss: 0.0633 - val_acc: 0.9797\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.97890 to 0.97970, saving model to weights_best.hdf5\n",
            "Epoch 12/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0984 - acc: 0.9690 - val_loss: 0.0572 - val_acc: 0.9819\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.97970 to 0.98190, saving model to weights_best.hdf5\n",
            "Epoch 13/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0939 - acc: 0.9710 - val_loss: 0.0544 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.98190 to 0.98220, saving model to weights_best.hdf5\n",
            "Epoch 14/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0902 - acc: 0.9720 - val_loss: 0.0518 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.98220\n",
            "Epoch 15/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0841 - acc: 0.9734 - val_loss: 0.0503 - val_acc: 0.9824\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.98220 to 0.98240, saving model to weights_best.hdf5\n",
            "Epoch 16/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0822 - acc: 0.9744 - val_loss: 0.0485 - val_acc: 0.9853\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.98240 to 0.98530, saving model to weights_best.hdf5\n",
            "Epoch 17/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0772 - acc: 0.9760 - val_loss: 0.0447 - val_acc: 0.9851\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.98530\n",
            "Epoch 18/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0753 - acc: 0.9764 - val_loss: 0.0419 - val_acc: 0.9865\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.98530 to 0.98650, saving model to weights_best.hdf5\n",
            "Epoch 19/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0730 - acc: 0.9770 - val_loss: 0.0427 - val_acc: 0.9845\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.98650\n",
            "Epoch 20/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0702 - acc: 0.9785 - val_loss: 0.0428 - val_acc: 0.9861\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.98650\n",
            "Epoch 21/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0669 - acc: 0.9790 - val_loss: 0.0404 - val_acc: 0.9863\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.98650\n",
            "Epoch 22/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0658 - acc: 0.9797 - val_loss: 0.0412 - val_acc: 0.9857\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.98650\n",
            "Epoch 23/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0648 - acc: 0.9793 - val_loss: 0.0408 - val_acc: 0.9868\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.98650 to 0.98680, saving model to weights_best.hdf5\n",
            "Epoch 24/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0628 - acc: 0.9801 - val_loss: 0.0369 - val_acc: 0.9888\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.98680 to 0.98880, saving model to weights_best.hdf5\n",
            "Epoch 25/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0601 - acc: 0.9812 - val_loss: 0.0350 - val_acc: 0.9885\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.98880\n",
            "Epoch 26/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0601 - acc: 0.9811 - val_loss: 0.0368 - val_acc: 0.9868\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.98880\n",
            "Epoch 27/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0564 - acc: 0.9824 - val_loss: 0.0353 - val_acc: 0.9883\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.98880\n",
            "Epoch 28/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0558 - acc: 0.9818 - val_loss: 0.0314 - val_acc: 0.9884\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.98880\n",
            "Epoch 29/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0544 - acc: 0.9828 - val_loss: 0.0419 - val_acc: 0.9864\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.98880\n",
            "Epoch 30/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0548 - acc: 0.9828 - val_loss: 0.0332 - val_acc: 0.9888\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.98880\n",
            "Epoch 31/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0528 - acc: 0.9830 - val_loss: 0.0368 - val_acc: 0.9881\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.98880\n",
            "Epoch 32/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0521 - acc: 0.9831 - val_loss: 0.0304 - val_acc: 0.9901\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.98880 to 0.99010, saving model to weights_best.hdf5\n",
            "Epoch 33/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0510 - acc: 0.9835 - val_loss: 0.0305 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99010\n",
            "Epoch 34/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0484 - acc: 0.9848 - val_loss: 0.0296 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.99010 to 0.99100, saving model to weights_best.hdf5\n",
            "Epoch 35/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0487 - acc: 0.9842 - val_loss: 0.0285 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99100\n",
            "Epoch 36/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0469 - acc: 0.9851 - val_loss: 0.0297 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99100\n",
            "Epoch 37/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0465 - acc: 0.9853 - val_loss: 0.0303 - val_acc: 0.9892\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99100\n",
            "Epoch 38/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0453 - acc: 0.9858 - val_loss: 0.0289 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99100\n",
            "Epoch 39/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0443 - acc: 0.9855 - val_loss: 0.0263 - val_acc: 0.9907\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99100\n",
            "Epoch 40/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0423 - acc: 0.9863 - val_loss: 0.0253 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00040: val_acc improved from 0.99100 to 0.99160, saving model to weights_best.hdf5\n",
            "Epoch 41/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0423 - acc: 0.9862 - val_loss: 0.0275 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.99160\n",
            "Epoch 42/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0418 - acc: 0.9865 - val_loss: 0.0268 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.99160\n",
            "Epoch 43/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0412 - acc: 0.9869 - val_loss: 0.0274 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.99160\n",
            "Epoch 44/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0400 - acc: 0.9871 - val_loss: 0.0237 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00044: val_acc improved from 0.99160 to 0.99210, saving model to weights_best.hdf5\n",
            "Epoch 45/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0389 - acc: 0.9875 - val_loss: 0.0252 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.99210\n",
            "Epoch 46/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0395 - acc: 0.9871 - val_loss: 0.0283 - val_acc: 0.9903\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.99210\n",
            "Epoch 47/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0387 - acc: 0.9877 - val_loss: 0.0241 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.99210\n",
            "Epoch 48/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0370 - acc: 0.9880 - val_loss: 0.0246 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.99210\n",
            "Epoch 49/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0362 - acc: 0.9883 - val_loss: 0.0249 - val_acc: 0.9913\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.99210\n",
            "Epoch 50/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0369 - acc: 0.9880 - val_loss: 0.0238 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.99210\n",
            "Epoch 51/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0362 - acc: 0.9880 - val_loss: 0.0239 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00051: val_acc improved from 0.99210 to 0.99210, saving model to weights_best.hdf5\n",
            "Epoch 52/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0357 - acc: 0.9881 - val_loss: 0.0265 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.99210\n",
            "Epoch 53/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0343 - acc: 0.9889 - val_loss: 0.0233 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.99210\n",
            "Epoch 54/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0351 - acc: 0.9885 - val_loss: 0.0218 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00054: val_acc improved from 0.99210 to 0.99270, saving model to weights_best.hdf5\n",
            "Epoch 55/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0346 - acc: 0.9887 - val_loss: 0.0215 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.99270\n",
            "Epoch 56/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0325 - acc: 0.9895 - val_loss: 0.0224 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.99270\n",
            "Epoch 57/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0337 - acc: 0.9891 - val_loss: 0.0209 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.99270\n",
            "Epoch 58/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0346 - acc: 0.9890 - val_loss: 0.0220 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.99270\n",
            "Epoch 59/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0327 - acc: 0.9893 - val_loss: 0.0214 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.99270\n",
            "Epoch 60/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0316 - acc: 0.9901 - val_loss: 0.0201 - val_acc: 0.9926\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.99270\n",
            "Epoch 61/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0319 - acc: 0.9894 - val_loss: 0.0235 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.99270\n",
            "Epoch 62/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0315 - acc: 0.9897 - val_loss: 0.0195 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00062: val_acc improved from 0.99270 to 0.99350, saving model to weights_best.hdf5\n",
            "Epoch 63/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0308 - acc: 0.9897 - val_loss: 0.0202 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.99350\n",
            "Epoch 64/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0306 - acc: 0.9898 - val_loss: 0.0199 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.99350\n",
            "Epoch 65/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0321 - acc: 0.9895 - val_loss: 0.0240 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.99350\n",
            "Epoch 66/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0305 - acc: 0.9899 - val_loss: 0.0222 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.99350\n",
            "Epoch 67/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0293 - acc: 0.9904 - val_loss: 0.0226 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.99350\n",
            "Epoch 68/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0288 - acc: 0.9903 - val_loss: 0.0175 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00068: val_acc improved from 0.99350 to 0.99400, saving model to weights_best.hdf5\n",
            "Epoch 69/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0288 - acc: 0.9905 - val_loss: 0.0199 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.99400\n",
            "Epoch 70/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0283 - acc: 0.9907 - val_loss: 0.0182 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.99400\n",
            "Epoch 71/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0297 - acc: 0.9902 - val_loss: 0.0208 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.99400\n",
            "Epoch 72/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0283 - acc: 0.9906 - val_loss: 0.0223 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.99400\n",
            "Epoch 73/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0279 - acc: 0.9909 - val_loss: 0.0203 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.99400\n",
            "Epoch 74/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0275 - acc: 0.9909 - val_loss: 0.0206 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00074: val_acc improved from 0.99400 to 0.99410, saving model to weights_best.hdf5\n",
            "Epoch 75/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0288 - acc: 0.9906 - val_loss: 0.0212 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.99410\n",
            "Epoch 76/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0272 - acc: 0.9911 - val_loss: 0.0224 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.99410\n",
            "Epoch 77/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0269 - acc: 0.9912 - val_loss: 0.0185 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00077: val_acc improved from 0.99410 to 0.99410, saving model to weights_best.hdf5\n",
            "Epoch 78/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0262 - acc: 0.9914 - val_loss: 0.0192 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.99410\n",
            "Epoch 79/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0259 - acc: 0.9917 - val_loss: 0.0242 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.99410\n",
            "Epoch 80/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0275 - acc: 0.9908 - val_loss: 0.0203 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.99410\n",
            "Epoch 81/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0267 - acc: 0.9913 - val_loss: 0.0197 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.99410\n",
            "Epoch 82/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0273 - acc: 0.9910 - val_loss: 0.0179 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.99410\n",
            "Epoch 83/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0254 - acc: 0.9918 - val_loss: 0.0199 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.99410\n",
            "Epoch 84/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0248 - acc: 0.9918 - val_loss: 0.0180 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00084: val_acc improved from 0.99410 to 0.99460, saving model to weights_best.hdf5\n",
            "Epoch 85/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0237 - acc: 0.9921 - val_loss: 0.0204 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.99460\n",
            "Epoch 86/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0259 - acc: 0.9913 - val_loss: 0.0183 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.99460\n",
            "Epoch 87/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0241 - acc: 0.9921 - val_loss: 0.0215 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.99460\n",
            "Epoch 88/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0249 - acc: 0.9917 - val_loss: 0.0162 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.99460\n",
            "Epoch 89/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0246 - acc: 0.9917 - val_loss: 0.0167 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.99460\n",
            "Epoch 90/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0248 - acc: 0.9916 - val_loss: 0.0185 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.99460\n",
            "Epoch 91/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0247 - acc: 0.9916 - val_loss: 0.0182 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.99460\n",
            "Epoch 92/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0250 - acc: 0.9915 - val_loss: 0.0213 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.99460\n",
            "Epoch 93/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0230 - acc: 0.9923 - val_loss: 0.0199 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.99460\n",
            "Epoch 94/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0239 - acc: 0.9921 - val_loss: 0.0166 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.99460\n",
            "Epoch 95/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0234 - acc: 0.9924 - val_loss: 0.0175 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.99460\n",
            "Epoch 96/150\n",
            "102000/102000 [==============================] - 2s 16us/step - loss: 0.0244 - acc: 0.9921 - val_loss: 0.0185 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.99460\n",
            "Epoch 97/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0233 - acc: 0.9923 - val_loss: 0.0171 - val_acc: 0.9951\n",
            "\n",
            "Epoch 00097: val_acc improved from 0.99460 to 0.99510, saving model to weights_best.hdf5\n",
            "Epoch 98/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0231 - acc: 0.9924 - val_loss: 0.0160 - val_acc: 0.9949\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.99510\n",
            "Epoch 99/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0231 - acc: 0.9923 - val_loss: 0.0182 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.99510\n",
            "Epoch 100/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0212 - acc: 0.9929 - val_loss: 0.0180 - val_acc: 0.9949\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.99510\n",
            "Epoch 101/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0225 - acc: 0.9921 - val_loss: 0.0170 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.99510\n",
            "Epoch 102/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0220 - acc: 0.9926 - val_loss: 0.0177 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.99510\n",
            "Epoch 103/150\n",
            "102000/102000 [==============================] - 2s 16us/step - loss: 0.0225 - acc: 0.9924 - val_loss: 0.0160 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.99510\n",
            "Epoch 104/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0234 - acc: 0.9922 - val_loss: 0.0191 - val_acc: 0.9935\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.99510\n",
            "Epoch 105/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0225 - acc: 0.9925 - val_loss: 0.0166 - val_acc: 0.9944\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.99510\n",
            "Epoch 106/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0214 - acc: 0.9925 - val_loss: 0.0173 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.99510\n",
            "Epoch 107/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0215 - acc: 0.9928 - val_loss: 0.0168 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.99510\n",
            "Epoch 108/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0213 - acc: 0.9928 - val_loss: 0.0188 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.99510\n",
            "Epoch 109/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0208 - acc: 0.9928 - val_loss: 0.0159 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.99510\n",
            "Epoch 110/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0211 - acc: 0.9930 - val_loss: 0.0155 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00110: val_acc improved from 0.99510 to 0.99520, saving model to weights_best.hdf5\n",
            "Epoch 111/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0195 - acc: 0.9934 - val_loss: 0.0163 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.99520\n",
            "Epoch 112/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0217 - acc: 0.9928 - val_loss: 0.0177 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.99520\n",
            "Epoch 113/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0206 - acc: 0.9930 - val_loss: 0.0173 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.99520\n",
            "Epoch 114/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0210 - acc: 0.9928 - val_loss: 0.0194 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.99520\n",
            "Epoch 115/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0205 - acc: 0.9931 - val_loss: 0.0179 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.99520\n",
            "Epoch 116/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0197 - acc: 0.9934 - val_loss: 0.0165 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.99520\n",
            "Epoch 117/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0204 - acc: 0.9931 - val_loss: 0.0164 - val_acc: 0.9958\n",
            "\n",
            "Epoch 00117: val_acc improved from 0.99520 to 0.99580, saving model to weights_best.hdf5\n",
            "Epoch 118/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0201 - acc: 0.9935 - val_loss: 0.0146 - val_acc: 0.9950\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.99580\n",
            "Epoch 119/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0196 - acc: 0.9934 - val_loss: 0.0174 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.99580\n",
            "Epoch 120/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0202 - acc: 0.9933 - val_loss: 0.0159 - val_acc: 0.9951\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.99580\n",
            "Epoch 121/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0188 - acc: 0.9940 - val_loss: 0.0158 - val_acc: 0.9955\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.99580\n",
            "Epoch 122/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0197 - acc: 0.9934 - val_loss: 0.0165 - val_acc: 0.9949\n",
            "\n",
            "Epoch 00122: val_acc did not improve from 0.99580\n",
            "Epoch 123/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0196 - acc: 0.9934 - val_loss: 0.0150 - val_acc: 0.9950\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.99580\n",
            "Epoch 124/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0185 - acc: 0.9935 - val_loss: 0.0194 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.99580\n",
            "Epoch 125/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0201 - acc: 0.9928 - val_loss: 0.0169 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.99580\n",
            "Epoch 126/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0208 - acc: 0.9929 - val_loss: 0.0167 - val_acc: 0.9950\n",
            "\n",
            "Epoch 00126: val_acc did not improve from 0.99580\n",
            "Epoch 127/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0198 - acc: 0.9933 - val_loss: 0.0177 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.99580\n",
            "Epoch 128/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0187 - acc: 0.9938 - val_loss: 0.0162 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.99580\n",
            "Epoch 129/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0191 - acc: 0.9938 - val_loss: 0.0161 - val_acc: 0.9955\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.99580\n",
            "Epoch 130/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0174 - acc: 0.9941 - val_loss: 0.0130 - val_acc: 0.9963\n",
            "\n",
            "Epoch 00130: val_acc improved from 0.99580 to 0.99630, saving model to weights_best.hdf5\n",
            "Epoch 131/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0186 - acc: 0.9937 - val_loss: 0.0153 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00131: val_acc did not improve from 0.99630\n",
            "Epoch 132/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0183 - acc: 0.9937 - val_loss: 0.0166 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00132: val_acc did not improve from 0.99630\n",
            "Epoch 133/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0188 - acc: 0.9935 - val_loss: 0.0150 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.99630\n",
            "Epoch 134/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0184 - acc: 0.9938 - val_loss: 0.0159 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.99630\n",
            "Epoch 135/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0189 - acc: 0.9938 - val_loss: 0.0140 - val_acc: 0.9961\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.99630\n",
            "Epoch 136/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0176 - acc: 0.9940 - val_loss: 0.0163 - val_acc: 0.9953\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.99630\n",
            "Epoch 137/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0183 - acc: 0.9940 - val_loss: 0.0148 - val_acc: 0.9956\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 0.99630\n",
            "Epoch 138/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0181 - acc: 0.9937 - val_loss: 0.0158 - val_acc: 0.9951\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 0.99630\n",
            "Epoch 139/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0182 - acc: 0.9938 - val_loss: 0.0175 - val_acc: 0.9949\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.99630\n",
            "Epoch 140/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0188 - acc: 0.9936 - val_loss: 0.0163 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.99630\n",
            "Epoch 141/150\n",
            "102000/102000 [==============================] - 1s 15us/step - loss: 0.0176 - acc: 0.9941 - val_loss: 0.0152 - val_acc: 0.9958\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.99630\n",
            "Epoch 142/150\n",
            "102000/102000 [==============================] - 1s 14us/step - loss: 0.0185 - acc: 0.9937 - val_loss: 0.0168 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 0.99630\n",
            "Epoch 143/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0177 - acc: 0.9940 - val_loss: 0.0166 - val_acc: 0.9950\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.99630\n",
            "Epoch 144/150\n",
            "102000/102000 [==============================] - 2s 16us/step - loss: 0.0174 - acc: 0.9942 - val_loss: 0.0171 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.99630\n",
            "Epoch 145/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0185 - acc: 0.9937 - val_loss: 0.0147 - val_acc: 0.9958\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 0.99630\n",
            "Epoch 146/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0187 - acc: 0.9938 - val_loss: 0.0150 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.99630\n",
            "Epoch 147/150\n",
            "102000/102000 [==============================] - 2s 16us/step - loss: 0.0168 - acc: 0.9943 - val_loss: 0.0168 - val_acc: 0.9947\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 0.99630\n",
            "Epoch 148/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0172 - acc: 0.9943 - val_loss: 0.0195 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.99630\n",
            "Epoch 149/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0181 - acc: 0.9937 - val_loss: 0.0159 - val_acc: 0.9956\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 0.99630\n",
            "Epoch 150/150\n",
            "102000/102000 [==============================] - 2s 15us/step - loss: 0.0179 - acc: 0.9940 - val_loss: 0.0152 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 0.99630\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#test_loss, test_acc = model.evaluate(test_imagesKeras, test_labelsKeras)\\nprint(\"_\"*80)\\nprint(\"Accuracy on test \", test_acc)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTZyhUrmx7qS",
        "colab_type": "text"
      },
      "source": [
        "# Result - We achieved accuracy greater than 99.4% with only 7926 number of parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWQMTPRLyTy9",
        "colab_type": "text"
      },
      "source": [
        "# Plotting the accuracy vs loss,epoch graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKHCtiJsUggK",
        "colab_type": "code",
        "outputId": "edf53827-2633-4290-ac10-9d987f45eab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "plt.plot(final_model.history['acc'])\n",
        "plt.plot(final_model.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'Val'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(final_model.history['loss'])\n",
        "plt.plot(final_model.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'Val'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSCgIIYSIkVAQQggRI6EghBAi5v8BOFL7Z8rBE94AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaqcIgfIWobo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}